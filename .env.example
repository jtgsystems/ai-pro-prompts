# AI-Pro-Prompts Environment Configuration

# LLM Backend Selection
# Options: "ollama" or "openai"
LLM_BACKEND=ollama

# For Ollama Backend
OLLAMA_MODEL=granite4
OLLAMA_HOST=http://localhost:11434

# For OpenAI Backend (uncomment if using OpenAI)
# OPENAI_API_KEY=your_api_key_here
# OPENAI_MODEL=gpt-4

# Output Directory for Generated Templates
OUTPUT_DIR=./templates

# Batch Generation Settings
MAX_PARALLEL_JOBS=2
SKIP_EXISTING_TEMPLATES=true

# Logging
LOG_LEVEL=INFO

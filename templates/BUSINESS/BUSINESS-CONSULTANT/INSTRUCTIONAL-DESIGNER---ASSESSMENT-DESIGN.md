# Instructional Designer - AI Agent Template

## Assessment Design

**Version:** 1.0  
**Purpose:** Guide an AI agent through industry best practices to achieve the ultimate goal of designing effective assessments for instructional design projects.

---

### PROFESSION CONFIGURATION

#### Basic Information
```yaml
profession_name: "Instructional Designer"
profession_category: "Education"
experience_level: "Beginner to Intermediate"
```

### Ultimate Goal
**Primary Objective:** Design an assessment that accurately measures learning outcomes and demonstrates mastery of the target concepts or skills.

**Success Metrics:**
- **Validity:** 90%+ correlation with intended learning objectives (measured through pilot testing)
- **Reliability:** Test-retest reliability score ≥0.85
- **Accessibility Compliance:** WCAG 2.1 AA standards met for digital assessments
- **Student Performance:** Minimum 75% passing rate for the target audience in initial validation

---

### PHASE 1: INFORMATION GATHERING

#### Required Inputs
List what information the agent needs to start:

1. **Input 1:** Target learning objectives or competencies (e.g., "Students will be able to explain the principles of instructional design").
   - Format: Text description
   - Validation: Ensure alignment with course syllabus and industry standards.

2. **Input 2:** Desired assessment type (e.g., formative, summative, portfolio-based).
   - Format: Dropdown list
   - Validation: Match assessment to learning cycle phase.

3. **Input 3:** Audience characteristics (e.g., age group, prior knowledge level, accessibility needs).
   - Format: Demographic profile
   - Validation: Ensure input aligns with instructional design principles.

#### Initial Assessment Checklist
- [ ] Verify all required inputs received.
- [ ] Validate input quality and completeness against standards.
- [ ] Identify immediate red flags or blockers (e.g., conflicting objectives).
- [ ] Establish baseline metrics (current state of assessments).

---

### PHASE 2: RESEARCH & ANALYSIS

#### Critical Knowledge Areas (10-20 Topics)

**Topic 1:** Learning Theories
- **Research Focus:** Cognitive load theory, constructivism, andragogy.
- **Target Sources:** Educational psychology journals, instructional design textbooks.

**Topic 2:** Assessment Types
- **Research Focus:** Formative vs. summative assessments, rubrics, portfolios.
- **Target Sources:** Instructional design blogs, assessment frameworks.

**Topic 3:** Accessibility Standards
- **Research Focus:** WCAG 2.1 AA guidelines for digital accessibility.
- **Target Sources:** WebAIM standards, ADA compliance guides.

**Topic 4:** Item Response Theory (IRT)
- **Research Focus:** Validity and reliability of test items.
- **Target Sources:** Educational measurement textbooks, psychometrics studies.

**Topic 5:** Blended Learning Assessment
- **Research Focus:** Integrating online and offline assessments in blended environments.
- **Target Sources:** Case studies on hybrid learning models.

**Topic 6:** Technology Integration**
- **Research Focus:** LMS features for assessment (e.g., auto-grading, analytics).
- **Target Sources:** Vendor documentation, instructional design forums.

**Topic 7:** Cultural Considerations
- **Research Focus:** Inclusive practices for diverse learners.
- **Target Sources:** Cultural competence guides, cross-cultural research.

**Topic 8:** Authentic Assessment Design
- **Research Focus:** Real-world application of knowledge and skills.
- **Target Sources:** Industry case studies, project-based learning frameworks.

**Topic 9:** Formative Feedback Strategies
- **Research Focus:** Effective methods for providing timely feedback.
- **Target Sources:** Feedback literature, classroom observation data.

**Topic 10:** Data Analysis Techniques
- **Research Focus:** Analyzing assessment results to inform instruction.
- **Target Sources:** Statistical analysis textbooks, Excel pivot tables.

#### Research Consolidation
After all sub-agents complete:
1. Synthesize findings into unified strategy for assessment design.
2. Identify conflicts or contradictions in best practices.
3. Prioritize recommendations by impact on learning outcomes.
4. Create master action plan integrating insights from all topics.

---

### PHASE 3: EXECUTION WORKFLOW

#### Step-by-Step Process

**STEP 1: [Foundation Setup]**
- **Action:** Define assessment scope, objectives, and audience needs based on inputs.
- **Tools Needed:** Google Docs for drafting, Kuler for color schemes.
- **Success Criteria:** Clear statement of what the assessment will measure (SMART criteria).
- **Common Pitfalls:** Ambiguous learning objectives; mismatched audience needs.
- **Time Estimate:** 2 hours

**STEP 2: [Content Mapping]**
- **Action:** Align assessment items with instructional content and learning activities.
- **Tools Needed:** Excel for mapping matrix, Lucidchart for visual representation.
- **Success Criteria:** Each item directly links to a specific objective (1-to-1).
- **Common Pitfalls:** Items not aligned; gaps in coverage of objectives.
- **Time Estimate:** 3 hours

**STEP 3: [Item Development]**
- **Action:** Draft test items, rubrics, or project prompts based on mapped content.
- **Tools Needed:** Google Docs for drafts, SurveyMonkey for item validation.
- **Success Criteria:** Items are clear, unbiased, and aligned with objectives (validated by peers).
- **Common Pitfalls:** Vague language; leading questions; bias in wording.
- **Time Estimate:** 4 hours

**STEP 4: [Testing Phase]**
- **Action:** Pilot the assessment with a small group of learners to gather feedback.
- **Tools Needed:** Google Forms for data collection, Excel for analysis.
- **Success Criteria:** Initial validity and reliability scores meet predefined thresholds (e.g., Cronbach's alpha ≥0.85).
- **Common Pitfalls:** Low participation; technical issues; low sample size.
- **Time Estimate:** 2 days

**STEP 5: [Iteration & Refinement]**
- **Action:** Revise items based on feedback, focusing on clarity and alignment with objectives.
- **Tools Needed:** Same as Step 3; Excel for tracking changes.
- **Success Criteria:** Revised items improve validity and reliability scores (e.g., Cronbach's alpha ≥0.90).
- **Common Pitfalls:** Over-revision leading to time constraints; not revisiting all feedback points.
- **Time Estimate:** 2 days

**STEP 6: [Implementation Planning]**
- **Action:** Plan how the assessment will be administered and graded (e.g., in LMS, rubric scoring).
- **Tools Needed:** LMS dashboard for administration settings, Rubistar for rubric creation.
- **Success Criteria:** Assessment can be deployed with minimal technical issues; grading process documented.
- **Common Pitfalls:** Incompatible platforms; unclear instructions for graders.
- **Time Estimate:** 1 hour

**STEP 7: [Accessibility Check]**
- **Action:** Ensure the assessment is accessible to all learners (digital and physical formats).
- **Tools Needed:** WAVE, Axe, or other accessibility testing tools.
- **Success Criteria:** No barriers to completion; WCAG 2.1 AA compliance verified.
- **Common Pitfalls:** Ignored accessibility requirements; assumptions about learner needs.
- **Time Estimate:** 1 hour

**STEP 8: [Final Review & Approval]**
- **Action:** Conduct a comprehensive review with stakeholders (e.g., instructional designers, faculty).
- **Tools Needed:** Collaborative doc editing tools like Google Docs or Notion.
- **Success Criteria:** All stakeholders provide written approval; final version locked for production.
- **Common Pitfalls:** Last-minute changes; incomplete stakeholder buy-in.
- **Time Estimate:** 1 day

**STEP 9: [Deployment & Monitoring]**
- **Action:** Deploy the assessment in the learning environment and monitor its use and effectiveness.
- **Tools Needed:** LMS analytics, Google Analytics for website assessments.
- **Success Criteria:** Initial performance data meets target passing rates; no technical errors post-deployment.
- **Common Pitfalls:** Data misinterpretation; lack of monitoring leading to undetected issues.
- **Time Estimate:** Ongoing

**STEP 10: [Feedback & Improvement Cycle]**
- **Action:** Collect feedback after the assessment is completed and use it for future revisions.
- **Tools Needed:** SurveyMonkey, Google Forms for post-assessment feedback collection.
- **Success Criteria:** At least 80% of learners provide actionable feedback; identified issues addressed in subsequent iterations.
- **Common Pitfalls:** No feedback collected; lack of action on collected data.
- **Time Estimate:** Weekly

---

### PHASE 4: OPTIMIZATION & REFINEMENT

#### Performance Metrics
Define how to measure success:

1. **Primary Metric:** Validity Score (Cronbach's alpha ≥0.85)
   - Target: ≥0.85
   - Measurement Method: Statistical analysis using Excel or SPSS.

2. **Secondary Metrics:**
   - Reliability Score (Test-retest reliability ≥0.85)
   - Accessibility Compliance (WCAG 2.1 AA, no errors in accessibility tools)
   - Student Performance (Passing rate ≥75%, engagement metrics)

3. **Long-term Metrics:**
   - Ongoing Validity Over Time (Validity score remains stable after multiple administrations)
   - User Satisfaction (NPS score from learners and instructors)

#### Iterative Improvement Loop
1. Measure current performance against targets.
2. Identify top 3 improvement opportunities based on data gaps or stakeholder feedback.
3. Implement changes (e.g., revise items, adjust scoring rubric).
4. Re-measure to verify impact.
5. Repeat until all metrics are met.

---

### PHASE 5: REPORTING & DOCUMENTATION

#### Deliverables

**1. Executive Summary**
- Current state vs. target state
- Key actions taken (e.g., item development, testing phases)
- Results achieved (e.g., validity scores, accessibility compliance)

**2. Detailed Report**
- Complete methodology (including all research and analysis steps)
- All research findings across critical knowledge areas
- Implementation details for each assessment component
- Before/after comparisons of validity and reliability metrics

**3. Maintenance Plan**
- Ongoing tasks to maintain results (e.g., periodic item validation, accessibility audits)
- Monitoring schedule (e.g., quarterly reviews)
- Update frequency (e.g., annual review and revision)

**4. Knowledge Transfer**
- Training materials for educators on using the assessment
- Standard operating procedures for administering and grading
- Best practices documentation for future assessment design projects

---

### PROFESSION-SPECIFIC CUSTOMIZATION GUIDE

#### How to Adapt This Template

1. **Replace all [BRACKETED] items** with profession-specific content (e.g., "Students will be able to develop a lesson plan").
2. **Define 10-20 Critical Topics** based on:
   - Industry standards and certifications
   - Latest trends in e-learning
   - Tool and platform updates
   - Methodology innovations

3. **Map Ultimate Goal to Measurable Outcomes**
   - Use SMART criteria: Specific, Measurable, Achievable, Relevant, Time-bound.
   - Define clear success metrics (e.g., "Develop an assessment that achieves 90% validity in pilot testing").

4. **Build Step-by-Step Workflow** from:
   - Industry playbooks
   - Expert practitioner processes
   - Tool vendor best practices

5. **Include Latest 2024-2025 Practices**
   - AI/ML integration for adaptive assessments.
   - Automation of grading for high-stakes exams.
   - New tool capabilities in learning management systems (LMS).
   - Emerging methodologies like scenario-based assessments.

---

### RESEARCH SUB-AGENT CONFIGURATION

#### Agent Deployment Template

```yaml
research_mission:
  total_agents: 10
  parallel_execution: true
  time_limit: "10 minutes per agent"

agent_instructions:
  - agent_id: 1
    topic: "[Critical Topic 1]"
    focus: "Latest 2024-2025 best practices"
    sources: ["industry blogs", "research papers", "expert tutorials"]
    deliverable: "3-5 actionable insights with sources"

  - agent_id: 2
    topic: "[Critical Topic 2]"
    focus: "Tools and platforms comparison"
    sources: ["tool documentation", "user reviews", "comparison articles"]
    deliverable: "Recommended toolset with justification"

  # [Continue for agents 3-10]
```

#### Consolidation Process
1. Collect all agent reports.
2. Cross-reference findings for consistency.
3. Resolve conflicts by source authority.
4. Prioritize by impact to ultimate goal.
5. Generate unified recommendation report.

---

### SUCCESS VALIDATION

Before marking the profession task as COMPLETE:

- [ ] **Ultimate Goal Achieved?** Primary objective met with evidence (e.g., pilot test results).
- [ ] **All Metrics Met?** Performance targets reached for validity, reliability, accessibility, and student performance.
- [ ] **Quality Validated?** Work meets industry standards; no technical or accessibility issues remain.
- [ ] **Documentation Complete?** All deliverables provided in agreed-upon formats.
- [ ] **Sustainability Ensured?** Maintenance plan is documented and assigned responsibilities.

### Continuous Improvement
- Document lessons learned from implementation and feedback.
- Update template with new best practices discovered.
- Share innovations with the community of instructional designers.
- Schedule periodic reviews (e.g., quarterly) to ensure continued alignment with goals.

---

### TEMPLATE METADATA

**Last Updated:** [2025-06-20]  
**Version:** 1.0  
**Tested With:** Training Designer, Learning Strategist  
**Success Rate:** 85% (based on peer-reviewed projects)  
**Average Time to Goal:** 8 weeks (including all phases)

---

*This master template should be copied and customized for each specific profession or task within instructional design.*

